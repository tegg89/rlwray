{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synchronous PPO with PyBullet Ant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packaged loaded. TF version is [1.15.0].\n"
     ]
    }
   ],
   "source": [
    "import datetime,gym,os,pybullet_envs,time,os,psutil,ray\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from util import gpu_sess,suppress_tf_warning,PPOBuffer,save_ppo_model_and_buffer,restore_ppo_model_and_buffer\n",
    "from ppo import create_ppo_model,create_ppo_graph\n",
    "np.set_printoptions(precision=2)\n",
    "suppress_tf_warning() # suppress warning \n",
    "gym.logger.set_level(40) # gym logger \n",
    "print (\"Packaged loaded. TF version is [%s].\"%(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rollout Worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env():\n",
    "    import pybullet_envs,gym\n",
    "    gym.logger.set_level(40) # gym logger \n",
    "    return gym.make('AntBulletEnv-v0')\n",
    "\n",
    "def get_eval_env():\n",
    "    import pybullet_envs,gym\n",
    "    gym.logger.set_level(40) # gym logger\n",
    "    eval_env = gym.make('AntBulletEnv-v0')\n",
    "#     _ = eval_env.render(mode='human') # enable rendering\n",
    "    _ = eval_env.reset()\n",
    "    for _ in range(3): # dummy run for proper rendering \n",
    "        a = eval_env.action_space.sample()\n",
    "        o,r,d,_ = eval_env.step(a)\n",
    "        time.sleep(0.01)\n",
    "    return eval_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.signal\n",
    "\n",
    "def discount_cumsum(x, discount):\n",
    "    \"\"\"\n",
    "    magic from rllab for computing discounted cumulative sums of vectors.\n",
    "    input: \n",
    "        vector x, \n",
    "        [x0, \n",
    "         x1, \n",
    "         x2]\n",
    "    output:\n",
    "        [x0 + discount * x1 + discount^2 * x2,  \n",
    "         x1 + discount * x2,\n",
    "         x2]\n",
    "    \"\"\"\n",
    "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "hdims = [64,64]\n",
    "# graph\n",
    "clip_ratio = 0.2\n",
    "lr = 3e-4\n",
    "ent_coef = 0.01\n",
    "vf_coef = 0.5\n",
    "max_grad_norm = 0.5\n",
    "# buffer\n",
    "steps_per_epoch = 5000\n",
    "gamma = 0.99\n",
    "lam = 0.95\n",
    "# update\n",
    "train_iters = 100\n",
    "target_kl = 0.01\n",
    "epochs = 1000\n",
    "max_ep_len = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutWorkerClass(object):\n",
    "    \"\"\"\n",
    "    Worker without RAY (for update purposes)\n",
    "    \"\"\"\n",
    "    def __init__(self,seed=1):\n",
    "        self.seed = seed\n",
    "        # Each worker should maintain its own environment\n",
    "#         import pybullet_envs,gym\n",
    "        from util import suppress_tf_warning\n",
    "        suppress_tf_warning() # suppress TF warnings\n",
    "        gym.logger.set_level(40) # gym logger \n",
    "        # self.env = gym.make('AntBulletEnv-v0')\n",
    "        self.env = get_env()\n",
    "        odim,adim = self.env.observation_space.shape[0],self.env.action_space.shape[0]\n",
    "        self.odim = odim\n",
    "        self.adim = adim\n",
    "        \n",
    "        # Create PPO model and computational graph \n",
    "        self.model,self.sess = create_ppo_model(env=self.env,hdims=hdims)\n",
    "        self.graph = create_ppo_graph(\n",
    "            self.model,lr=lr,clip_ratio=clip_ratio,\n",
    "            ent_coef=ent_coef,vf_coef=vf_coef,max_grad_norm = max_grad_norm)\n",
    "        \n",
    "        # Initialize model \n",
    "        tf.set_random_seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def get_action(self,o,deterministic=False):\n",
    "        act_op = self.model['mu'] if deterministic else self.model['pi']\n",
    "        return self.sess.run(act_op, feed_dict={self.model['o_ph']:o.reshape(1,-1)})[0]\n",
    "\n",
    "    def get_weights(self):\n",
    "        \"\"\"\n",
    "        Get weights\n",
    "        \"\"\"\n",
    "        weight_vals = self.sess.run(self.model['pi_vars'] + self.model['v_vars'])\n",
    "        return weight_vals\n",
    " \n",
    "            \n",
    "@ray.remote\n",
    "class RayRolloutWorkerClass(object):\n",
    "    \"\"\"\n",
    "    Rollout Worker with RAY\n",
    "    \"\"\"\n",
    "    def __init__(self,worker_id=0,ep_len_rollout=1000):\n",
    "        # Parse\n",
    "        self.worker_id = worker_id\n",
    "        self.ep_len_rollout = ep_len_rollout\n",
    "        # Each worker should maintain its own environment\n",
    "        # import pybullet_envs,gym\n",
    "        from util import suppress_tf_warning\n",
    "        suppress_tf_warning() # suppress TF warnings\n",
    "        gym.logger.set_level(40) # gym logger \n",
    "        # self.env = gym.make('AntBulletEnv-v0')\n",
    "        self.env = get_env()\n",
    "        odim,adim = self.env.observation_space.shape[0],self.env.action_space.shape[0]\n",
    "        self.odim = odim\n",
    "        self.adim = adim\n",
    "        \n",
    "        # Create PPO model\n",
    "        self.model,self.sess = create_ppo_model(env=self.env,hdims=hdims)\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.buf = PPOBuffer(odim=odim, adim=adim, size=ep_len_rollout, gamma=gamma, lam=lam)\n",
    "        print (\"Ray Worker [%d] Ready.\"%(self.worker_id))\n",
    "        \n",
    "        # Flag to initialize assign operations for 'set_weights()'\n",
    "        self.FIRST_SET_FLAG = True\n",
    "        \n",
    "        # Flag to initialize rollout\n",
    "        self.FIRST_ROLLOUT_FLAG = True\n",
    "        \n",
    "    def get_action(self,o,deterministic=False):\n",
    "        act_op = self.model['mu'] if deterministic else self.model['pi']\n",
    "        return self.sess.run(act_op, feed_dict={self.model['o_ph']:o.reshape(1,-1)})[0]\n",
    "                                    \n",
    "    def set_weights(self,weight_vals):\n",
    "        \"\"\"\n",
    "        Set weights without memory leakage\n",
    "        \"\"\"\n",
    "        if self.FIRST_SET_FLAG:\n",
    "            self.FIRST_SET_FLAG = False\n",
    "            self.assign_placeholders = []\n",
    "            self.assign_ops = []\n",
    "            for w_idx,weight_tf_var in enumerate(self.model['pi_vars'] + self.model['v_vars']):\n",
    "                a = weight_tf_var\n",
    "                assign_placeholder = tf.placeholder(a.dtype, shape=a.get_shape())\n",
    "                assign_op = a.assign(assign_placeholder)\n",
    "                self.assign_placeholders.append(assign_placeholder)\n",
    "                self.assign_ops.append(assign_op)\n",
    "                \n",
    "        for w_idx,weight_tf_var in enumerate(self.model['pi_vars'] + self.model['v_vars']):\n",
    "            self.sess.run(self.assign_ops[w_idx],\n",
    "                          {self.assign_placeholders[w_idx]:weight_vals[w_idx]})\n",
    "            \n",
    "    def rollout(self):\n",
    "        \"\"\"\n",
    "        Rollout\n",
    "        \"\"\"\n",
    "        if self.FIRST_ROLLOUT_FLAG:\n",
    "            self.FIRST_ROLLOUT_FLAG = False\n",
    "            self.o = self.env.reset() # reset environment\n",
    "            \n",
    "        # Loop\n",
    "        for t in range(ep_len_rollout):\n",
    "            a,v_t,logp_t = self.sess.run(\n",
    "                self.model['get_action_ops'],feed_dict={self.model['o_ph']:self.o.reshape(1,-1)})\n",
    "            o2,r,d,_ = self.env.step(a[0])\n",
    "            \n",
    "            self.buf.store(self.o, a, r, v_t, logp_t)\n",
    "            \n",
    "            # Save next state \n",
    "            self.o = o2\n",
    "            \n",
    "            if d:\n",
    "                self.buf.finish_path(last_val=0.)\n",
    "                self.o = self.env.reset() # reset when done\n",
    "        \n",
    "        last_val = self.sess.run(self.model['v'], feed_dict={self.model['o_ph']:self.o.reshape(1,-1)})\n",
    "        \n",
    "        self.buf.finish_path(last_val=last_val)\n",
    "        \n",
    "        return self.buf.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vb = np.zeros((100,1))\n",
    "# ab = np.zeros((100,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = [[0.96776974,-0.7034082,0.5083786,0.06832138,0.2833113,0.32790965,0.8092029,-0.29485628]]\n",
    "# v_t = [0.00756513]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vb[t,:] = v_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ab[0:3,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initilize PyBullet Ant Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "n_cpu = n_workers = 3\n",
    "total_steps,evaluate_every,print_every = 300,5,1\n",
    "ep_len_rollout = 5000\n",
    "batch_size,update_count = 128,ep_len_rollout\n",
    "num_eval,max_ep_len_eval = 3,1e3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env = get_eval_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-02 18:20:45,743\tINFO resource_spec.py:212 -- Starting Ray with 4.98 GiB memory available for workers and up to 10.0 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-07-02 18:20:46,095\tINFO services.py:1170 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I] Created trainer\n",
      "RAY initialized with [3] cpus and [3] workers.\n"
     ]
    }
   ],
   "source": [
    "ray.init(num_cpus=n_cpu,\n",
    "         memory = 5*1024*1024*1024,\n",
    "         object_store_memory = 10*1024*1024*1024,\n",
    "         driver_object_store_memory = 1*1024*1024*1024)\n",
    "tf.reset_default_graph()\n",
    "R = RolloutWorkerClass(seed=1)\n",
    "workers = [RayRolloutWorkerClass.remote(\n",
    "    worker_id=i,ep_len_rollout=ep_len_rollout) \n",
    "           for i in range(n_workers)]\n",
    "print (\"RAY initialized with [%d] cpus and [%d] workers.\"%\n",
    "       (n_cpu,n_workers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43923)\u001b[0m Ray Worker [2] Ready.\n",
      "\u001b[2m\u001b[36m(pid=43924)\u001b[0m Ray Worker [1] Ready.\n",
      "\u001b[2m\u001b[36m(pid=43925)\u001b[0m Ray Worker [0] Ready.\n",
      "[1/300] rollout:[9.4]s update:[1.9]s.\n",
      "[Evaluate] step:[1/300][0.0%] #step:[0.0e+00] time:[00:00:11] ram:[65.1%].\n",
      "[Evaluate] ep_ret:[278.6957] ep_len:[1000]\n",
      "[2/300] rollout:[8.1]s update:[1.6]s.\n",
      "[3/300] rollout:[7.6]s update:[1.7]s.\n",
      "[4/300] rollout:[7.7]s update:[1.8]s.\n",
      "[5/300] rollout:[7.9]s update:[1.8]s.\n",
      "[Evaluate] step:[5/300][1.3%] #step:[0.0e+00] time:[00:00:51] ram:[67.5%].\n",
      "[Evaluate] ep_ret:[586.4084] ep_len:[1000]\n",
      "[6/300] rollout:[8.3]s update:[1.8]s.\n",
      "[7/300] rollout:[8.6]s update:[1.9]s.\n",
      "[8/300] rollout:[12.6]s update:[2.3]s.\n",
      "[9/300] rollout:[11.6]s update:[2.2]s.\n",
      "[10/300] rollout:[11.2]s update:[2.1]s.\n",
      "[Evaluate] step:[10/300][3.0%] #step:[0.0e+00] time:[00:01:55] ram:[68.1%].\n",
      "[Evaluate] ep_ret:[23.5409] ep_len:[33]\n",
      "[11/300] rollout:[9.9]s update:[1.9]s.\n",
      "[12/300] rollout:[9.5]s update:[1.9]s.\n",
      "[13/300] rollout:[10.2]s update:[2.0]s.\n",
      "[14/300] rollout:[11.3]s update:[2.1]s.\n",
      "[15/300] rollout:[10.5]s update:[2.0]s.\n",
      "[Evaluate] step:[15/300][4.7%] #step:[0.0e+00] time:[00:02:56] ram:[69.0%].\n",
      "[Evaluate] ep_ret:[615.4799] ep_len:[1000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-61252a51e475>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mt_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mworker\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mrollout_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0msec_rollout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ray/lib/python3.6/site-packages/ray/worker.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(object_ids, timeout)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[0;32mglobal\u001b[0m \u001b[0mlast_task_error_raise_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1507\u001b[0m         \u001b[0;31m# TODO(ujvl): Consider how to allow user to retrieve the ready objects.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1508\u001b[0;31m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1509\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRayError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ray/lib/python3.6/site-packages/ray/worker.py\u001b[0m in \u001b[0;36mget_objects\u001b[0;34m(self, object_ids, timeout)\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0mtimeout_ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         data_metadata_pairs = self.core_worker.get_objects(\n\u001b[0;32m--> 306\u001b[0;31m             object_ids, self.current_task_id, timeout_ms)\n\u001b[0m\u001b[1;32m    307\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeserialize_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_metadata_pairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.CoreWorker.get_objects\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "n_env_step = 0 # number of environment steps\n",
    "\n",
    "for t in range(int(total_steps)):\n",
    "    esec = time.time()-start_time\n",
    "    \n",
    "    # Synchronize worker weights\n",
    "    weights = R.get_weights()\n",
    "    set_weights_list = [worker.set_weights.remote(weights) for worker in workers] \n",
    "    \n",
    "    # Make rollout and accumulate to Buffers\n",
    "    t_start = time.time()\n",
    "    ops = [worker.rollout.remote() for worker in workers]\n",
    "    rollout_vals = ray.get(ops)\n",
    "    sec_rollout = time.time() - t_start\n",
    "    \n",
    "    # Get stats before update\n",
    "    t_start = time.time()\n",
    "    feeds_list = []\n",
    "    for rollout_val in rollout_vals:\n",
    "        feeds = {k:v for k,v in zip(R.model['all_phs'],rollout_val)}\n",
    "        feeds_list.append(feeds)\n",
    "        pi_l_old, v_l_old, ent = R.sess.run(\n",
    "            [R.graph['pi_loss'],R.graph['v_loss'],R.graph['approx_ent']],feed_dict=feeds)\n",
    "        \n",
    "    # Update the central agent\n",
    "    for i in range(train_iters):\n",
    "        for r_idx,rollout_val in enumerate(rollout_vals):\n",
    "            feeds = feeds_list[r_idx]\n",
    "            _, kl = R.sess.run([R.graph['train_op'], R.graph['approx_kl']], feed_dict=feeds)\n",
    "            if kl > 1.5 * target_kl:\n",
    "                break\n",
    "            \n",
    "    # Get stats after update\n",
    "    for r_idx,rollout_val in enumerate(rollout_vals):\n",
    "        feeds = feeds_list[r_idx]\n",
    "        pi_l_new,v_l_new,kl,cf = R.sess.run(\n",
    "            [R.graph['pi_loss'],R.graph['v_loss'],R.graph['approx_kl'],R.graph['clipfrac']],\n",
    "            feed_dict=feeds)\n",
    "    sec_update = time.time() - t_start\n",
    "\n",
    "    # Print\n",
    "    if (t == 0) or (((t+1)%print_every) == 0): \n",
    "        print (\"[%d/%d] rollout:[%.1f]s update:[%.1f]s.\"%\n",
    "               (t+1,total_steps,sec_rollout,sec_update))\n",
    "\n",
    "    # Evaluate\n",
    "    if (t == 0) or (((t+1)%evaluate_every) == 0) or (t == (total_steps-1)): \n",
    "        ram_percent = psutil.virtual_memory().percent # memory usage\n",
    "        print (\"[Evaluate] step:[%d/%d][%.1f%%] #step:[%.1e] time:[%s] ram:[%.1f%%].\"%\n",
    "               (t+1,total_steps,t/total_steps*100,\n",
    "                n_env_step,\n",
    "                time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-start_time)),\n",
    "                ram_percent))\n",
    "        \n",
    "        o,d,ep_ret,ep_len = eval_env.reset(),False,0,0\n",
    "#         _ = eval_env.render(mode='human') \n",
    "        while not(d or (ep_len == max_ep_len_eval)):\n",
    "            a = R.sess.run(R.model['mu'],feed_dict={R.model['o_ph']:o.reshape(1,-1)})\n",
    "            o,r,d,_ = eval_env.step(a[0])\n",
    "#             _ = eval_env.render(mode='human') \n",
    "            ep_ret += r # compute return \n",
    "            ep_len += 1\n",
    "        print (\"[Evaluate] ep_ret:[%.4f] ep_len:[%d]\"%(ep_ret,ep_len)) \n",
    "            \n",
    "#         # Save current SAC model and replay buffers \n",
    "#         npz_path = '../data/net/pybullet_ant/ppo_model_and_buffers.npz'\n",
    "#         save_ppo_model_and_buffer(npz_path,R,ppobuf,VERBOSE=False)\n",
    "    \n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model weights and replay buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to save the npz file \n",
    "npz_path = '../data/net/pybullet_ant/ppo_model_and_buffers_final.npz'\n",
    "save_ppo_model_and_buffer(npz_path,R,ppobuf,VERBOSE=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reset the worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R.sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and assign model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load npz\n",
    "npz_path = '../data/net/pybullet_ant/model_and_buffers_final.npz'\n",
    "restore_ppo_model_and_buffer(npz_path,R,ppobuf,VERBOSE=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test-Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env = get_eval_env()\n",
    "o,d,ep_ret,ep_len = eval_env.reset(),False,0,0\n",
    "_ = eval_env.render(mode='human') \n",
    "while not(d or (ep_len == max_ep_len_eval)):\n",
    "    a = R.get_action(o,deterministic=True)\n",
    "    o,r,d,_ = eval_env.step(a)\n",
    "    _ = eval_env.render(mode='human') \n",
    "    ep_ret += r # compute return \n",
    "    ep_len += 1\n",
    "print (\"[Evaluate] ep_ret:[%.4f] ep_len:[%d]\"\n",
    "    %(ep_ret,ep_len))\n",
    "eval_env.close() # close env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ray",
   "language": "python",
   "name": "ray"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
